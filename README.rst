|test| |codecov| |docs|

.. |test| image:: https://github.com/intsystems/ProjectTemplate/workflows/test/badge.svg
    :target: https://github.com/intsystems/ProjectTemplate/tree/master
    :alt: Test status
    
.. |codecov| image:: https://img.shields.io/codecov/c/github/intsystems/ProjectTemplate/master
    :target: https://app.codecov.io/gh/intsystems/ProjectTemplate
    :alt: Test coverage
    
.. |docs| image:: https://github.com/intsystems/ProjectTemplate/workflows/docs/badge.svg
    :target: https://intsystems.github.io/ProjectTemplate/
    :alt: Docs status


.. class:: center

    :Название исследуемой задачи: Дистилляция моделей на многодоменных выборках
    :Тип научной работы: M1P
    :Автор: Алексей Станиславович Орлов
    :Научный руководитель: степень, Грабовой Андрей

Abstract
========

The paper investigates the problem of reducing the complexity of the approximating model when transferred to new data of lower power. The concepts of a teacher and a student are introduced for different data sets. Methods based on the distillation of machine learning models, including the Bayesian approach, are considered. An assumption is introduced that solving the optimization problem from the parameters of both models and domains improves the quality of the student's model. A computational experiment is being carried out on different data for regression, classification and text recognition problems.

Research publications
===============================
1. 

Presentations at conferences on the topic of research
================================================
1. 

Software modules developed as part of the study
======================================================
1. A python package *mylib* with all implementation `here <https://github.com/intsystems/ProjectTemplate/tree/master/src>`_.
2. A code with all experiment visualisation `here <https://github.comintsystems/ProjectTemplate/blob/master/code/main.ipynb>`_. Can use `colab <http://colab.research.google.com/github/intsystems/ProjectTemplate/blob/master/code/main.ipynb>`_.
